import os
import time
from datetime import datetime
import openpyxl

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

# ========== CONFIG ==========
# Input and output Excel files
INPUT_EXCEL  = r"C:\Users\atharsa\Downloads\LILO AND STITCH INPUT.xlsx"
OUTPUT_EXCEL = r"C:\Users\atharsa\Downloads\LILO AND STITCH OUTPUT.xlsx"

# Choose how to run Chrome:
ATTACH_TO_EXISTING_CHROME = True     # True -> attach to Chrome you started with the BAT
DEBUGGER_ADDRESS = "127.0.0.1:9222"  # matches the BAT

# If you prefer the script to launch Chrome instead, set ATTACH_TO_EXISTING_CHROME = False
# and configure a dedicated profile to persist login:
USER_DATA_DIR = r"C:\Users\atharsa\ChromeSelenium"   # change if you like
PROFILE_DIR   = "Default"

# Save frequency (save workbook after this many input rows/pages)
AUTOSAVE_EVERY = 1

# ============================

def load_input_workbook(path):
    if not os.path.exists(path):
        raise FileNotFoundError(f"Input Excel not found: {path}")
    wb = openpyxl.load_workbook(path)
    return wb, wb.active

def load_or_create_output_workbook(path):
    if os.path.exists(path):
        wb = openpyxl.load_workbook(path)
        # first sheet holds the data
        sheet_data = wb.active
        # progress sheet logs which page URLs are done
        if "__progress__" in wb.sheetnames:
            sheet_progress = wb["__progress__"]
        else:
            sheet_progress = wb.create_sheet("__progress__")
            sheet_progress.append(["Row", "Source ID", "Page URL", "Images Found", "Status", "Finished At"])
    else:
        wb = openpyxl.Workbook()
        sheet_data = wb.active
        sheet_data.title = "Data"
        sheet_data.append(["Source ID", "Image URL"])
        sheet_progress = wb.create_sheet("__progress__")
        sheet_progress.append(["Row", "Source ID", "Page URL", "Images Found", "Status", "Finished At"])
    return wb, sheet_data, sheet_progress

def build_seen_pairs(sheet_data):
    """Build a set of (source_id_str, image_url) that already exist in output to avoid duplicates."""
    seen = set()
    for row in sheet_data.iter_rows(min_row=2, values_only=True):
        if not row:
            continue
        src, img = row[0], row[1] if len(row) > 1 else None
        if src is None or img is None:
            continue
        seen.add((str(src), str(img)))
    return seen

def build_processed_pages(sheet_progress):
    """Pages that were fully processed earlier (so we can skip them)."""
    processed = set()
    for row in sheet_progress.iter_rows(min_row=2, values_only=True):
        if not row:
            continue
        page_url = row[2]
        status = (row[4] or "").upper()
        if page_url and status == "DONE":
            processed.add(page_url)
    return processed

def safe_save_workbook(wb, target_path, retries=1):
    """
    Try to save to target_path. If it's locked (e.g., open in Excel),
    write to an autosave file so we never lose progress.
    """
    try:
        wb.save(target_path)
        return target_path
    except PermissionError as e:
        # Fallback autosave
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        alt = target_path.replace(".xlsx", f".autosave_{ts}.xlsx")
        wb.save(alt)
        print(f"⚠️ Output file in use. Wrote to autosave: {alt}")
        return alt

def make_driver():
    chrome_options = Options()

    if ATTACH_TO_EXISTING_CHROME:
        # Attach to a Chrome that YOU started with remote debugging + anti-throttle flags
        chrome_options.add_experimental_option("debuggerAddress", DEBUGGER_ADDRESS)
    else:
        # Script launches Chrome itself with persistent profile and anti-throttle flags
        chrome_options.add_argument(f"--user-data-dir={USER_DATA_DIR}")
        chrome_options.add_argument(f"--profile-directory={PROFILE_DIR}")
        # Flags to keep work going in background/minimized
        chrome_options.add_argument("--disable-renderer-backgrounding")
        chrome_options.add_argument("--disable-background-timer-throttling")
        chrome_options.add_argument("--disable-backgrounding-occluded-windows")
        # If you want *no* window at all, uncomment next line (works well for scraping):
        # chrome_options.add_argument("--headless=new")

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    # Keep it; not required to be focused.
    try:
        driver.maximize_window()
    except Exception:
        pass
    return driver

def wait_for_page_settled(driver, timeout_s=15):
    """Basic readyState wait; dynamic content is handled by our scrolling below."""
    end = time.time() + timeout_s
    while time.time() < end:
        state = driver.execute_script("return document.readyState")
        if state == "complete":
            return
        time.sleep(0.2)

def scroll_to_bottom(driver, wait_between_scrolls=1.0, stable_rounds=3, max_rounds=60):
    """
    Scrolls until page height stops growing for `stable_rounds` checks,
    or until `max_rounds` is hit.
    """
    last_height = 0
    stable = 0
    rounds = 0
    while True:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(wait_between_scrolls)
        new_height = driver.execute_script(
            "return Math.max(document.body.scrollHeight, document.documentElement.scrollHeight);"
        )
        rounds += 1
        if new_height <= last_height:
            stable += 1
        else:
            stable = 0
            last_height = new_height
        if stable >= stable_rounds or rounds >= max_rounds:
            break

def extract_image_urls(driver):
    """
    Collects image URLs considering src/currentSrc and common lazy-loading attributes.
    Filters out data: URIs.
    """
    js = """
    const urls = new Set();
    document.querySelectorAll('img').forEach(img => {
        const candidates = [
            img.src,
            img.currentSrc,
            img.getAttribute('src'),
            img.getAttribute('data-src'),
            img.getAttribute('data-lazy'),
            img.getAttribute('data-original')
        ];
        for (const c of candidates) {
            if (c && !String(c).startsWith('data:')) { urls.add(c); break; }
        }
    });
    return Array.from(urls);
    """
    return driver.execute_script(js) or []

def maybe_canonicalize(url):
    """
    Optional site-specific cleanup for URLs that contain '._' (e.g., thumbnail variants).
    Keep simple and safe; fall back to original if anything looks off.
    """
    try:
        if "._" in url:
            # Try to preserve extension
            dot = url.rfind(".")
            if dot != -1 and dot >= len(url) - 6:
                return url.split("._")[0] + url[dot:]
    except Exception:
        pass
    return url

def main():
    # Load Excel files
    wb_in, sheet_in = load_input_workbook(INPUT_EXCEL)
    wb_out, sheet_out, sheet_prog = load_or_create_output_workbook(OUTPUT_EXCEL)

    # Build de-duplication and resume helpers
    seen_pairs = build_seen_pairs(sheet_out)
    processed_pages = build_processed_pages(sheet_prog)

    driver = make_driver()

    pages_since_save = 0
    try:
        for row_idx in range(2, sheet_in.max_row + 1):
            try:
                source_id = sheet_in[f"A{row_idx}"].value
                page_url  = sheet_in[f"B{row_idx}"].value
                if not page_url:
                    continue

                if page_url in processed_pages:
                    print(f"Row {row_idx}: already processed, skipping.")
                    continue

                driver.get(page_url)
                wait_for_page_settled(driver, timeout_s=15)
                # Scroll to trigger lazy loads
                scroll_to_bottom(driver, wait_between_scrolls=1.0, stable_rounds=3, max_rounds=60)

                raw_urls = extract_image_urls(driver)
                unique_urls = set(maybe_canonicalize(u) for u in raw_urls if u)

                # Append only new (SourceID, ImageURL) pairs
                written = 0
                for img_url in unique_urls:
                    key = (str(source_id), img_url)
                    if key in seen_pairs:
                        continue
                    sheet_out.append([source_id, img_url])
                    seen_pairs.add(key)
                    written += 1

                # Log progress for this page (even if 0 new; it's complete)
                sheet_prog.append([
                    row_idx, source_id, page_url, written, "DONE",
                    datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                ])
                processed_pages.add(page_url)

                print(f"Row {row_idx}: found {len(unique_urls)} images; wrote {written} new.")

                pages_since_save += 1
                if pages_since_save >= AUTOSAVE_EVERY:
                    safe_save_workbook(wb_out, OUTPUT_EXCEL)
                    pages_since_save = 0

            except Exception as e:
                # Log failure for this row; don't stop the whole job
                sheet_prog.append([
                    row_idx, sheet_in[f"A{row_idx}"].value, sheet_in[f"B{row_idx}"].value,
                    0, f"ERROR: {e}", datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                ])
                safe_save_workbook(wb_out, OUTPUT_EXCEL)
                print(f"Row {row_idx}: Error - {e}")

    except KeyboardInterrupt:
        print("⚠️ Interrupted by user. Saving what we have...")
    finally:
        safe_save_workbook(wb_out, OUTPUT_EXCEL)
        try:
            driver.quit()
        except Exception:
            pass
        print("✅ Script complete.")

if __name__ == "__main__":
    main()